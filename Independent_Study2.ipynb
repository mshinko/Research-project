{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUiFafL3hasKzinDRb7UW6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshinko/Research-project/blob/main/Independent_Study2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL52Vep1VHiD",
        "outputId": "a0b2dc0a-459c-4c69-aab6-c8608823a7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=85289bb2cbd1808e2dde08883ec5eb3b1c32f7bf0819e16c5f17df3a6d75c6d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: tokenizers, safetensors, xxhash, dill, rouge-score, multiprocess, huggingface-hub, transformers, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.17.2 multiprocess-0.70.15 rouge-score-0.1.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets transformers rouge-score nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojfgx03kumHD",
        "outputId": "1873644d-bc68-43e3-f39f-4fc40aac601b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -u"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etPCutCOuz70",
        "outputId": "95b23ae6-874d-49b8-fb4c-f61f0cf99c8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgOOfMZtRE9P",
        "outputId": "ee7643f1-77ff-4625-d667-ebf3623dcbd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.33.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"t5-small\""
      ],
      "metadata": {
        "id": "S1uDw1_2RE-0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmysj4TARFIc",
        "outputId": "0a8a5287-abde-4b62-9024-b0e3e375c837"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/81.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.17.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.0 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jsonlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXw7ZA6OSaDi",
        "outputId": "6a15cb56-4671-457d-a04d-a8633888b297"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Use triple-quoted strings to preserve the formatting\n",
        "original_text = \"\"\"\n",
        "  To: VRSS Management Team\n",
        "    From: David Schlingloff\n",
        "    Subject: VRSS Management Report – Week ending January 30th, 2022\n",
        "    Copies: Management Team, Advisors\n",
        "\n",
        "    Priorities\n",
        "    We are working on these priorities:\n",
        "    1. Finalize a set of interested and motivated team members.\n",
        "    2. Define competitors to the VRSS product.\n",
        "    3. Identify possible methods of VR streaming.\n",
        "\n",
        "    Progress\n",
        "    During this (short) reporting period, we accomplished these tasks:\n",
        "    - Developed the idea and company:\n",
        "      - Brainstormed possible ideas to pursue and decided upon the VR service.\n",
        "      - Created a name for the company (Virtual Reality Sports Seating - VRSS).\n",
        "    - Founded the Team:\n",
        "      - Confirmed 4 teammates (we may accept more in the following days).\n",
        "      - Drafted our MOKRs for discussion.\n",
        "\n",
        "    Plans\n",
        "    During the next week, we plan to finish these tasks:\n",
        "    - Find 5 competitors to the planned VRSS service:\n",
        "      - Perform research online to develop 5 possible competitors (both direct and indirect) to VRSS service by Friday, February 4th (Alex, Ryan, Matt).\n",
        "    - Brainstorm 4 VR streaming-capable cameras:\n",
        "      - Find 4 different types of video cameras (preferably inexpensive, <$1000) that can support VR streaming capabilities by Friday, February 4th.\n",
        "      - Begin understanding the limitations of streaming bandwidth (David, Kartik).\n",
        "\n",
        "    Problems\n",
        "    We are aware of these problems:\n",
        "    - We need to finalize our team:\n",
        "      - There are still people who were interested in joining last week but left too early to make a final decision. We would like to complete this by the end of class on Monday, January 31st.\n",
        "    - We are unsure if cameras can support our bandwidth:\n",
        "      - The team would like to use a limited number of cameras that can be accessed by many different people during a game. The team is unsure if it is even possible to get these types of cameras without spending excessive amounts of money.\n",
        "    - We do not currently have experience with software/website creation:\n",
        "      - Much of the VRSS streaming service is dependent on a reliable stream with easy website access. A lot of our resources will need to be spent on attempting to create an appealing website with streaming capabilities.\n",
        "\n",
        "    Management Report.doc 2022Jan30 Page 1/2\n",
        "\n",
        "    Attachments\n",
        "    - None.\n",
        "\"\"\"\n",
        "\n",
        "# Define the summary\n",
        "summary = \"Week 1 Created the team and MOKR. Plan to find competitors and understand technology needed. They lack the knowledge to build a streaming service solution and it is unclear who their customers are that would pay for it. Identified problems but don’t have plans for mitigating.\"\n",
        "\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'Original Text': [original_text], 'Summary': [summary]})\n",
        "# Define the new row data as a dictionary\n",
        "new_row = {\n",
        "    'Original Text': \"\"\"\n",
        "    To: VRSS Management Team\n",
        "    From: David Schlingloff\n",
        "    Subject: VRSS Management Report – Week ending February 6th, 2022\n",
        "    Copies: Management Team, Advisors\n",
        "\n",
        "    Priorities\n",
        "    We are working on these priorities:\n",
        "    1. Finalize a set of interested and motivated team members.\n",
        "    2. Defining competitors to the VRSS product.\n",
        "    3. Identifying possible methods of VR streaming.\n",
        "\n",
        "    Progress\n",
        "    During this (short) reporting period, we accomplished these tasks:\n",
        "    - Find 5 competitors to the planned VRSS service – Developed 5 possible\n",
        "    competitors (both direct and indirect) to VRSS service.\n",
        "    - Brainstorm 4 VR streaming capable cameras - Found 4+ different types of\n",
        "    video cameras (preferably inexpensive, <$1000) that can support VR\n",
        "    streaming capabilities.\n",
        "\n",
        "    Plans\n",
        "    During the next week, we plan to finish these tasks:\n",
        "    - Refine our target consumers - Talk about refining the consumers we defined\n",
        "    last week. Build upon this by creating 3 theoretical customer profiles (David,\n",
        "    Alex, Matthew).\n",
        "    - Define our advisory board - We will think about the people that would be\n",
        "    helpful for us to talk to. Think about areas we could ask questions in. Using\n",
        "    these topics, search for people we can possibly talk to and discuss our project\n",
        "    plan. (Ryan, Entire Team).\n",
        "    - Find specific details about 3 competitors - From the competitors we found\n",
        "    last week, go on their websites and explore what they do and learn about\n",
        "    their history. Specifically, create a document by February 11th, with the\n",
        "    following 3 things noted for each: similarities/differences between company\n",
        "    and VRSS, the prices for that service, and if/how they advertise. (Kartik,\n",
        "    Ryan).\n",
        "\n",
        "    Problems\n",
        "    We are aware of these problems:\n",
        "    - We do not currently have experience with software / website creation –\n",
        "    Much of the VRSS streaming service is dependent on a reliable stream\n",
        "    with easy website access. A lot of our resources will need to be spent on\n",
        "    attempting to create an appealing website with streaming capabilities.\n",
        "    - Defining our place in the market is tricky - The team mapped out the\n",
        "    two existing markets - going to sports games and streaming them on TV.\n",
        "    We realized that there is a definitive space in the market for VR streaming,\n",
        "    but it might be tricky to convince the target consumers to pursue this\n",
        "    option versus the others.\n",
        "    - Finding VR capable equipment is expensive - VR is still a relatively new\n",
        "    type of streaming, and therefore the market for this technology is still\n",
        "    relatively niche. Finding equipment (like a VR capable camera) is possible,\n",
        "    but our requirement of <$1000 makes it difficult.\n",
        "    - Our marketing efforts/strategy is a current problem we have. In order to\n",
        "    acquire a lot of customers and produce sales, our marketing and\n",
        "    advertising will need to be phenomenal. We could develop a social media\n",
        "    account (Instagram/Facebook) and/or pay for a commercial\n",
        "    advertisement on TV.\n",
        "\n",
        "    Management Report.doc 2022Feb06 Page 1/5\n",
        "\n",
        "    Attachments\n",
        "    - None.\n",
        "    \"\"\",\n",
        "    'Summary': \"\"\"\n",
        "    Week 2\n",
        "    Found competitors and VR streaming cameras. Plan on analyzing competition, find advisory board members, and create 3 customer profiles. They lack the knowledge to build a streaming service solution, the equipment is too expensive and it is unclear who their customers are that would pay for it. Identified problems but don’t have plans for mitigating.\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "# Append the new row to the DataFrame\n",
        "df = df.append(new_row, ignore_index=True)\n",
        "# Define the new row\n",
        "new_row_2 = {\n",
        "    'Original Text': \"\"\"\n",
        "   To: VRSS Management Team\n",
        "From: David Schlingloff\n",
        "Subject: VRSS Management Report – Week ending February 13th, 2022\n",
        "Copies: Management Team, Advisors\n",
        "Priorities\n",
        "We are working on these priorities:\n",
        "1. Gauge the interest of VR equipment with our target markets\n",
        "2. Identify a way to live-stream in VR\n",
        "3. Brainstorm ways to begin website creation\n",
        "Progress\n",
        "During this reporting period, we accomplished these tasks:\n",
        "● Refined our target consumers - Developed our ideal target market by\n",
        "creating 3 theoretical customer profiles based on the 3 customer groups that\n",
        "the team developed.\n",
        "● Defined our advisory board - Listed nine people that the team would like to\n",
        "invite to the first project review meeting. Each person has different unique\n",
        "talents and abilities that could help the team move forward.\n",
        "● Scheduled VR equipment rental - Reserved a VR camera and VR headset at\n",
        "the Virginia Tech library so that the team can begin testing with the\n",
        "equipment.\n",
        "● Find specific details about 3 competitors - Unable to accomplish this in the\n",
        "week timeframe. The team decided that the information we already have is\n",
        "enough to move forward with and we spent our resources on other tasks.\n",
        "Plans\n",
        "During the next week, we plan to finish these tasks:\n",
        "● Create an interest survey - Use google forms to develop a survey that can\n",
        "gauge the interest in this service based on the 3 target markets defined by\n",
        "February 17th. Generate a list of 9 people (three from each target market) to\n",
        "be sent out by February 21st. (Kartik, Ryan, Alex)\n",
        "● Obtain and use VR equipment - Receive the VR camera and headset from\n",
        "the Virginia Tech library and attempt to connect to the computer. See if it is\n",
        "possible to first record a video in VR. Then work on streaming options if time\n",
        "permits for the week. The team will research and document the process for\n",
        "video recording and video streaming (if possible). (David, all team)\n",
        "\n",
        "Management Report.doc 2022Feb13 Page 1/3\n",
        "\n",
        "Problems\n",
        "We are aware of these problems:\n",
        "● We still do not currently have experience with software / website\n",
        "creation – Much of the VRSS streaming service is dependent on a reliable\n",
        "stream with easy website access. A lot of our resources will need to be\n",
        "spent on attempting to create an appealing website with streaming\n",
        "capabilities. However, we are now looking at meeting with people who do\n",
        "have this experience and developing a plan of integration.\n",
        "● VR equipment implementation requires resources - Although the team\n",
        "was successful in figuring out a way to rent VR equipment, it has realized\n",
        "that in order to record and stream video, a powerful and expensive\n",
        "computer is needed. It will need to be portable so that it can be moved to\n",
        "where the VR cameras are located.\n",
        "● Our marketing efforts/strategy - In order to acquire a lot of customers and\n",
        "produce sales, our marketing and advertising will need to be phenomenal.\n",
        "We could develop a social media account (instagram/facebook) and/or\n",
        "pay for\n",
        "    \"\"\",\n",
        "    'Summary': \"\"\"\n",
        "    Week 3\n",
        "    Created customer profiles, list of potential advisory board members, and reserved VR equipment rentals to address the expense constraint identified last week. Failed to analyze competitors they believe they have sufficient information to move forward though not clear what that is. They plan to create an interest survey (as opposed to interviewing as indicated in MOKR) and record a VR video with rented equipment. They lack the knowledge to build a streaming service solution, the equipment is too expensive. They continue to have the same problems but don’t have plans for mitigating. Will need to intervene to get them unstuck.\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "\n",
        "# Append the new row to the DataFrame\n",
        "df = df.append(new_row_2, ignore_index=True)\n",
        "new_row3 = {\n",
        "    'Original Text': \"\"\"\n",
        "    To: VRSS Management Team\n",
        "From: David Schlingloff\n",
        "Subject: VRSS Management Report – Week ending March 4th, 2022\n",
        "Copies: Management Team, Advisors\n",
        "Priorities\n",
        "We are working on these priorities:\n",
        "1. Get people to take our survey and evaluate the results.\n",
        "2. Identify a way to live-stream in VR\n",
        "3. Submit a video to KickstartVT\n",
        "Progress\n",
        "During this reporting period, we accomplished these tasks:\n",
        "● Analyzed interest survey results - Received 25 responses from the survey and\n",
        "wrote a one-page report summarizing the key-findings from the survey that was\n",
        "sent out. The team found that we should continue to pursue this product. (Matt,\n",
        "Ryan, Alex)\n",
        "● Obtained and use VR equipment - Obtained the Oculus Go and successfully\n",
        "connected it to YouTube VR. The process was documented and an Oculus Quest\n",
        "2 was scheduled for rent to attempt to do the same thing. If it works as well, then\n",
        "we will discuss using the Insta360 camera to live-stream to the Oculus Quest 2.\n",
        "(David, Kartik)\n",
        "● Performed individual project reviews - Gave the presentation to our advisory\n",
        "board members. (David, all team members)\n",
        "\n",
        "Plans\n",
        "During the next week, we plan to finish these tasks:\n",
        "● Finalize interest survey results - Synthesize the survey results and add to our\n",
        "report and project review presentation. (Ryan, Matt, Alex)\n",
        "● Obtain and use VR equipment - Obtain the Oculus Quest 2 and see if it can\n",
        "connect to YouTube VR. If so, document the process and start to discuss\n",
        "live-streaming options using the Insta360 camera. (Alex, Kartik)\n",
        "● Perform team project review - Revise presentation template for the class and\n",
        "present the project review to the team on Wednesday, March 16th. Record the\n",
        "comments from the Q&A. (David, all team members)\n",
        "● Create a video for KickStartVT - Record and edit a video for KickstartVT and\n",
        "submit on Sunday, March 20th. (David, all team members)\n",
        "\n",
        "Problems\n",
        "We are aware of these problems:\n",
        "● We can only rent equipment for a limited period - We would like to rent an\n",
        "Oculus Go or an Oculus Quest (since they are stand-alone devices), but they\n",
        "require rentals so timing can get delayed significantly.\n",
        "● We still do not currently have experience with software / website creation –\n",
        "Much of the VRSS streaming service is dependent on a reliable stream with easy\n",
        "website access. A lot of our resources will need to be spent on attempting to\n",
        "create an appealing website with streaming capabilities. However, we are now\n",
        "looking at meeting with people who do have this experience and developing a\n",
        "plan of integration.\n",
        "● Our marketing efforts/strategy - In order to acquire a lot of customers and\n",
        "produce sales, our marketing and advertising will need to be aggressive. We\n",
        "could develop a social media account (instagram/facebook) and/or pay for a\n",
        "commercial advertisement on TV.\n",
        "    \"\"\",\n",
        "    'Summary': \"\"\"\n",
        "  Week 5\n",
        "Sent out the previously created interest survey and tried out an HTC Vive but didn’t  function with Youtube App. Scheduled time with advisory board and potential team member. Plan to try an Oculus Go, analyze the survey data, and prepare for the project review. They still lack the knowledge to build a streaming service solution, the renting of the vr equipment is too burdensome  due to rental policies. They continue to have the same problems of website creation and marketing  but don’t have plans for mitigating. Will need to intervene to get them unstuck.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "}\n",
        "df = df.append(new_row3, ignore_index=True)\n",
        "# Print the updated DataFrame\n",
        "print(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw2RLKbeSaOI",
        "outputId": "dca84c5d-14d3-494d-a7dc-8421db54cbb8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       Original Text  \\\n",
            "0  \\n  To: VRSS Management Team\\n    From: David ...   \n",
            "1  \\n    To: VRSS Management Team\\n    From: Davi...   \n",
            "2  \\n   To: VRSS Management Team\\nFrom: David Sch...   \n",
            "3  \\n    To: VRSS Management Team\\nFrom: David Sc...   \n",
            "\n",
            "                                             Summary  \n",
            "0  Week 1 Created the team and MOKR. Plan to find...  \n",
            "1  \\n    Week 2\\n    Found competitors and VR str...  \n",
            "2  \\n    Week 3\\n    Created customer profiles, l...  \n",
            "3  \\n  Week 5\\nSent out the previously created in...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-86-f6790bda8678>:127: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(new_row, ignore_index=True)\n",
            "<ipython-input-86-f6790bda8678>:194: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(new_row_2, ignore_index=True)\n",
            "<ipython-input-86-f6790bda8678>:256: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(new_row3, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "xSx6UD6XRFLC"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Original Text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "vv-YGGk4RFRh",
        "outputId": "bc406e95-334e-484e-e3bc-04ef557e7e3c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  To: VRSS Management Team\\n    From: David Schlingloff\\n    Subject: VRSS Management Report – Week ending January 30th, 2022\\n    Copies: Management Team, Advisors\\n\\n    Priorities\\n    We are working on these priorities:\\n    1. Finalize a set of interested and motivated team members.\\n    2. Define competitors to the VRSS product.\\n    3. Identify possible methods of VR streaming.\\n\\n    Progress\\n    During this (short) reporting period, we accomplished these tasks:\\n    - Developed the idea and company:\\n      - Brainstormed possible ideas to pursue and decided upon the VR service.\\n      - Created a name for the company (Virtual Reality Sports Seating - VRSS).\\n    - Founded the Team:\\n      - Confirmed 4 teammates (we may accept more in the following days).\\n      - Drafted our MOKRs for discussion.\\n\\n    Plans\\n    During the next week, we plan to finish these tasks:\\n    - Find 5 competitors to the planned VRSS service:\\n      - Perform research online to develop 5 possible competitors (both direct and indirect) to VRSS service by Friday, February 4th (Alex, Ryan, Matt).\\n    - Brainstorm 4 VR streaming-capable cameras:\\n      - Find 4 different types of video cameras (preferably inexpensive, <$1000) that can support VR streaming capabilities by Friday, February 4th.\\n      - Begin understanding the limitations of streaming bandwidth (David, Kartik).\\n\\n    Problems\\n    We are aware of these problems:\\n    - We need to finalize our team:\\n      - There are still people who were interested in joining last week but left too early to make a final decision. We would like to complete this by the end of class on Monday, January 31st.\\n    - We are unsure if cameras can support our bandwidth:\\n      - The team would like to use a limited number of cameras that can be accessed by many different people during a game. The team is unsure if it is even possible to get these types of cameras without spending excessive amounts of money.\\n    - We do not currently have experience with software/website creation:\\n      - Much of the VRSS streaming service is dependent on a reliable stream with easy website access. A lot of our resources will need to be spent on attempting to create an appealing website with streaming capabilities.\\n\\n    Management Report.doc 2022Jan30 Page 1/2\\n\\n    Attachments\\n    - None.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBUMMx8xS0Hz",
        "outputId": "dd48aaa2-ef92-4ccc-a49d-bbd73189b8a5"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
              "Calculates average rouge scores for a list of hypotheses and references\n",
              "Args:\n",
              "    predictions: list of predictions to score. Each prediction\n",
              "        should be a string with tokens separated by spaces.\n",
              "    references: list of reference for each prediction. Each\n",
              "        reference should be a string with tokens separated by spaces.\n",
              "    rouge_types: A list of rouge types to calculate.\n",
              "        Valid names:\n",
              "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
              "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
              "        `\"rougeLsum\"`: rougeLsum splits text using `\"\n",
              "\"`.\n",
              "        See details in https://github.com/huggingface/datasets/issues/617\n",
              "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
              "    use_aggregator: Return aggregates if this is set to True\n",
              "Returns:\n",
              "    rouge1: rouge_1 (f1),\n",
              "    rouge2: rouge_2 (f1),\n",
              "    rougeL: rouge_l (f1),\n",
              "    rougeLsum: rouge_lsum (f1)\n",
              "Examples:\n",
              "\n",
              "    >>> rouge = evaluate.load('rouge')\n",
              "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
              "    >>> references = [\"hello there\", \"general kenobi\"]\n",
              "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "V5hr6Yq9S0Pt"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_checkpoint in [\"google/t5-small-lm-adapt\", \"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
        "  prefix = \"summarize: \"\n",
        "else:\n",
        "  prefix =\"\""
      ],
      "metadata": {
        "id": "3b6IdvfrS0T-"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(row):\n",
        "    doc = row[\"Original Text\"]\n",
        "    summary = row[\"Summary\"]\n",
        "\n",
        "    inputs = [prefix + doc]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(summary, max_length=max_target_length, truncation=True, padding=True)\n",
        "\n",
        "    # Flatten the input_ids and attention_mask\n",
        "    model_inputs = {key: [item for sublist in value for item in sublist] for key, value in model_inputs.items()}\n",
        "\n",
        "    # Flatten the labels\n",
        "    labels = [item for item in labels[\"input_ids\"]]\n",
        "\n",
        "    # Return a dictionary containing the model inputs and labels\n",
        "    return {\n",
        "        \"input_ids\": model_inputs[\"input_ids\"],\n",
        "        \"attention_mask\": model_inputs[\"attention_mask\"],\n",
        "        \"labels\": labels,\n",
        "    }"
      ],
      "metadata": {
        "id": "K-jX5QRhS0XQ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocess_function to your DataFrame\n",
        "preprocessed_data = df.apply(preprocess_function, axis=1)\n",
        "# Print the preprocessed data\n",
        "print(preprocessed_data)"
      ],
      "metadata": {
        "id": "QyH9yKsIUWoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc04bbc-fef2-466d-e026-a31c22d02802"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    {'input_ids': [21603, 10, 304, 10, 13893, 4256...\n",
            "1    {'input_ids': [21603, 10, 304, 10, 13893, 4256...\n",
            "2    {'input_ids': [21603, 10, 304, 10, 13893, 4256...\n",
            "3    {'input_ids': [21603, 10, 304, 10, 13893, 4256...\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split your DataFrame into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply the preprocess_function to both train and validation DataFrames\n",
        "train_tokenized = train_df.apply(preprocess_function, axis=1).tolist()\n",
        "val_tokenized = val_df.apply(preprocess_function, axis=1).tolist()\n",
        "\n",
        "# Create dictionary-based datasets\n",
        "tokenized_datasets = {'train': train_tokenized, 'validation': val_tokenized}\n",
        "\n",
        "# Print the tokenized datasets\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "bVkOvZrbVJ0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b95f3b-fc50-4eaf-ebfa-2deb2d6f15ff"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': [{'input_ids': [21603, 10, 304, 10, 13893, 4256, 2159, 2271, 1029, 10, 1955, 13675, 53, 40, 1647, 19237, 10, 13893, 4256, 2159, 3750, 3, 104, 6551, 7784, 1332, 314, 189, 6, 460, 2884, 638, 8497, 10, 2159, 2271, 6, 17228, 7, 6783, 2197, 101, 33, 464, 30, 175, 12697, 10, 1300, 1609, 151, 12, 240, 69, 3719, 11, 6825, 8, 772, 5, 1682, 3, 23393, 3, 9, 194, 12, 619, 18, 8103, 16, 13893, 1877, 3325, 1538, 3, 9, 671, 12, 20759, 10208, 18645, 24583, 3, 2092, 48, 5099, 1059, 6, 62, 10380, 175, 4145, 10, 3, 2, 5331, 120, 776, 26, 1046, 3719, 772, 3, 18, 24083, 26, 944, 7216, 45, 8, 3719, 11, 2832, 3, 9, 80, 18, 6492, 934, 4505, 1635, 2610, 8, 843, 18, 8954, 53, 7, 45, 8, 3719, 24, 47, 1622, 91, 5, 37, 372, 435, 24, 62, 225, 916, 12, 6665, 48, 556, 5, 41, 329, 144, 17, 6, 7826, 6, 5104, 61, 3, 2, 4249, 10733, 11, 169, 13893, 1277, 3, 18, 4249, 10733, 8, 411, 1497, 302, 1263, 11, 4234, 2895, 34, 12, 5343, 13893, 5, 37, 433, 47, 15552, 11, 46, 411, 1497, 302, 2415, 222, 204, 47, 5018, 21, 3170, 12, 3332, 12, 103, 8, 337, 589, 5, 156, 34, 930, 38, 168, 6, 258, 62, 56, 2497, 338, 8, 86, 2427, 19208, 1861, 12, 619, 18, 8103, 12, 8, 411, 1497, 302, 2415, 222, 1682, 41, 308, 9, 6961, 6, 4556, 4414, 61, 3, 2, 1915, 10816, 928, 516, 2456, 3, 18, 2776, 162, 8, 3831, 12, 69, 18599, 1476, 724, 5, 41, 308, 9, 6961, 6, 66, 372, 724, 61, 14423, 3, 2092, 8, 416, 471, 6, 62, 515, 12, 1992, 175, 4145, 10, 3, 2, 6514, 1737, 1046, 3719, 772, 3, 18, 8951, 532, 7991, 8, 3719, 772, 11, 617, 12, 69, 934, 11, 516, 1132, 3831, 5, 41, 448, 63, 152, 6, 5199, 6, 5104, 61, 3, 2, 3, 31635, 11, 169, 13893, 1277, 3, 18, 3, 31635, 8, 411, 1497, 302, 2415, 222, 204, 11, 217, 3, 99, 34, 54, 1979, 12, 5343, 13893, 5, 156, 78, 6, 1708, 8, 433, 11, 456, 12, 2497, 619, 18, 8103, 53, 931, 338, 8, 86, 2427, 19208, 1861, 5, 41, 27280, 6, 4556, 4414, 61, 3, 2, 22871, 372, 516, 1132, 3, 18, 6342, 159, 15, 3831, 3847, 21, 8, 853, 11, 915, 8, 516, 1132, 12, 8, 372, 30, 2875, 6, 1332, 898, 189, 5, 11392, 8, 2622, 45, 8, 1593, 184, 188, 5, 41, 308, 9, 6961, 6, 66, 372, 724, 61, 3, 2, 6357, 3, 9, 671, 21, 20759, 7681, 17, 18645, 3, 18, 11392, 11, 4777, 3, 9, 671, 21, 20759, 10208, 18645, 11, 4237, 30, 1771, 6, 1332, 460, 189, 5, 41, 308, 9, 6961, 6, 66, 372, 724, 61, 5289, 7, 101, 33, 2718, 13, 175, 982, 10, 3, 2, 101, 54, 163, 3170, 1277, 21, 3, 9, 1643, 1059, 3, 18, 101, 133, 114, 12, 3170, 46, 411, 1497, 302, 1263, 42, 46, 411, 1497, 302, 2415, 222, 41, 27296, 79, 33, 1518, 18, 138, 782, 1904, 201, 68, 79, 1457, 15981, 78, 9398, 54, 129, 16124, 4019, 5, 3, 2, 101, 341, 103, 59, 1083, 43, 351, 28, 889, 3, 87, 475, 3409, 3, 104, 8718, 13, 8, 13893, 4256, 8340, 313, 19, 8976, 30, 3, 9, 3468, 6093, 28, 514, 475, 592, 5, 71, 418, 13, 69, 1438, 56, 174, 12, 36, 1869, 30, 3, 12663, 12, 482, 46, 12817, 475, 28, 8340, 5644, 5, 611, 6, 62, 33, 230, 479, 44, 1338, 28, 151, 113, 103, 43, 48, 351, 11, 2421, 3, 9, 515, 13, 5660, 5, 3, 2, 421, 1070, 2231, 87, 7, 17, 2206, 122, 63, 3, 18, 86, 455, 12, 7464, 3, 9, 418, 13, 722, 11, 1759, 1085, 6, 69, 1070, 11, 3662, 56, 174, 12, 36, 8299, 5, 101, 228, 1344, 3, 9, 569, 783, 905, 41, 21246, 5096, 87, 4861, 2567, 61, 11, 87, 127, 726, 21, 3, 9, 1328, 21592, 30, 1424, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [6551, 305, 4892, 17, 91, 8, 3150, 990, 1046, 3719, 11, 1971, 91, 46, 19989, 1813, 162, 68, 737, 22, 17, 1681, 28, 18116, 2276, 5, 14890, 26, 97, 28, 18599, 1476, 11, 1055, 372, 1144, 5, 2926, 12, 653, 46, 411, 1497, 302, 1263, 6, 8341, 8, 3719, 331, 6, 11, 2967, 21, 8, 516, 1132, 5, 328, 341, 2136, 8, 1103, 12, 918, 3, 9, 8340, 313, 1127, 6, 8, 17992, 13, 8, 3, 208, 52, 1277, 19, 396, 9054, 5529, 788, 12, 3571, 3101, 5, 328, 916, 12, 43, 8, 337, 982, 13, 475, 3409, 11, 1070, 68, 278, 22, 17, 43, 1390, 21, 181, 12581, 1222, 5, 2003, 174, 12, 7770, 15, 12, 129, 135, 1149, 17, 4636, 5, 1]}, {'input_ids': [21603, 10, 304, 10, 13893, 4256, 2159, 2271, 1029, 10, 1955, 13675, 53, 40, 1647, 19237, 10, 13893, 4256, 2159, 3750, 3, 104, 6551, 7784, 1762, 604, 189, 6, 460, 2884, 638, 8497, 10, 2159, 2271, 6, 17228, 7, 6783, 2197, 101, 33, 464, 30, 175, 12697, 10, 1300, 6514, 1737, 3, 9, 356, 13, 1638, 11, 11361, 372, 724, 5, 1682, 374, 13536, 9216, 12, 8, 13893, 4256, 556, 5, 1877, 3, 23393, 487, 2254, 13, 13893, 8340, 5, 24583, 3, 2092, 48, 41, 7, 14184, 61, 5099, 1059, 6, 62, 10380, 175, 4145, 10, 3, 18, 3, 31192, 8, 800, 11, 349, 10, 3, 18, 14170, 7279, 2726, 487, 912, 12, 6665, 11, 1500, 1286, 8, 13893, 313, 5, 3, 18, 6357, 26, 3, 9, 564, 21, 8, 349, 41, 21031, 17, 3471, 23963, 5716, 15915, 53, 3, 18, 13893, 4256, 137, 3, 18, 3, 20100, 8, 2271, 10, 3, 18, 1193, 7001, 15, 26, 314, 24558, 41, 1123, 164, 1845, 72, 16, 8, 826, 477, 137, 3, 18, 21409, 15, 26, 69, 7109, 439, 448, 7, 21, 3071, 5, 14423, 3, 2092, 8, 416, 471, 6, 62, 515, 12, 1992, 175, 4145, 10, 3, 18, 2588, 305, 9216, 12, 8, 4355, 13893, 4256, 313, 10, 3, 18, 22871, 585, 367, 12, 1344, 305, 487, 9216, 41, 17158, 1223, 11, 16335, 61, 12, 13893, 4256, 313, 57, 1701, 6, 2083, 314, 189, 41, 27280, 6, 7826, 6, 5199, 137, 3, 18, 14170, 21556, 314, 13893, 8340, 18, 4010, 179, 7724, 10, 3, 18, 2588, 314, 315, 1308, 13, 671, 7724, 41, 21669, 13938, 6, 3, 2, 3229, 16824, 61, 24, 54, 380, 13893, 8340, 5644, 57, 1701, 6, 2083, 314, 189, 5, 3, 18, 10129, 77, 1705, 8, 10005, 13, 8340, 19703, 41, 308, 9, 6961, 6, 4556, 4414, 137, 5289, 7, 101, 33, 2718, 13, 175, 982, 10, 3, 18, 101, 174, 12, 804, 1737, 69, 372, 10, 3, 18, 290, 33, 341, 151, 113, 130, 1638, 16, 6109, 336, 471, 68, 646, 396, 778, 12, 143, 3, 9, 804, 1357, 5, 101, 133, 114, 12, 743, 48, 57, 8, 414, 13, 853, 30, 2089, 6, 1762, 2664, 7, 17, 5, 3, 18, 101, 33, 3, 20305, 3, 99, 7724, 54, 380, 69, 19703, 10, 3, 18, 37, 372, 133, 114, 12, 169, 3, 9, 1643, 381, 13, 7724, 24, 54, 36, 3, 11282, 57, 186, 315, 151, 383, 3, 9, 467, 5, 37, 372, 19, 3, 20305, 3, 99, 34, 19, 237, 487, 12, 129, 175, 1308, 13, 7724, 406, 2887, 10981, 6201, 13, 540, 5, 3, 18, 101, 103, 59, 1083, 43, 351, 28, 889, 87, 8398, 3585, 3409, 10, 3, 18, 8718, 13, 8, 13893, 4256, 8340, 313, 19, 8976, 30, 3, 9, 3468, 6093, 28, 514, 475, 592, 5, 71, 418, 13, 69, 1438, 56, 174, 12, 36, 1869, 30, 3, 12663, 12, 482, 46, 12817, 475, 28, 8340, 5644, 5, 2159, 3750, 5, 7171, 460, 2884, 683, 152, 1458, 5545, 7739, 28416, 4128, 3, 18, 14794, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [6551, 209, 6357, 26, 8, 372, 11, 7109, 439, 448, 5, 2926, 12, 253, 9216, 11, 734, 748, 906, 5, 328, 2136, 8, 1103, 12, 918, 3, 9, 8340, 313, 1127, 11, 34, 19, 19363, 113, 70, 722, 33, 24, 133, 726, 21, 34, 5, 3, 21153, 3676, 982, 68, 278, 22, 17, 43, 1390, 21, 181, 12581, 1222, 5, 1]}, {'input_ids': [21603, 10, 304, 10, 13893, 4256, 2159, 2271, 1029, 10, 1955, 13675, 53, 40, 1647, 19237, 10, 13893, 4256, 2159, 3750, 3, 104, 6551, 7784, 2083, 1179, 189, 6, 460, 2884, 638, 8497, 10, 2159, 2271, 6, 17228, 7, 6783, 2197, 101, 33, 464, 30, 175, 12697, 10, 1300, 12520, 397, 8, 1046, 13, 13893, 1277, 28, 69, 2387, 3212, 1682, 3, 23393, 3, 9, 194, 12, 619, 18, 8103, 16, 13893, 1877, 14170, 21556, 1155, 12, 1731, 475, 3409, 24583, 3, 2092, 48, 5099, 1059, 6, 62, 10380, 175, 4145, 10, 3, 2, 419, 13536, 26, 69, 2387, 3674, 3, 18, 3, 31192, 69, 1523, 2387, 512, 57, 1577, 220, 13605, 884, 10958, 3, 390, 30, 8, 220, 884, 1637, 24, 8, 372, 1597, 5, 3, 2, 374, 13536, 26, 69, 18599, 1476, 3, 18, 3, 28886, 4169, 151, 24, 8, 372, 133, 114, 12, 5484, 12, 8, 166, 516, 1132, 1338, 5, 1698, 568, 65, 315, 775, 12227, 11, 8075, 24, 228, 199, 8, 372, 888, 1039, 5, 3, 2, 14890, 26, 13893, 1277, 3571, 3, 18, 9473, 26, 3, 9, 13893, 1861, 11, 13893, 22545, 44, 8, 5382, 7130, 3595, 78, 24, 8, 372, 54, 1731, 2505, 28, 8, 1277, 5, 3, 2, 2588, 806, 1030, 81, 220, 9216, 3, 18, 597, 179, 12, 8582, 48, 16, 8, 471, 97, 11415, 5, 37, 372, 1500, 24, 8, 251, 62, 641, 43, 19, 631, 12, 888, 1039, 28, 11, 62, 1869, 69, 1438, 30, 119, 4145, 5, 14423, 3, 2092, 8, 416, 471, 6, 62, 515, 12, 1992, 175, 4145, 10, 3, 2, 6357, 46, 1046, 3719, 3, 18, 2048, 10283, 2807, 12, 1344, 3, 9, 3719, 24, 54, 13596, 8, 1046, 16, 48, 313, 3, 390, 30, 8, 220, 2387, 3212, 4802, 57, 2083, 1003, 189, 5, 6939, 2206, 3, 9, 570, 13, 668, 151, 41, 21182, 45, 284, 2387, 512, 61, 12, 36, 1622, 91, 57, 2083, 1401, 7, 17, 5, 41, 439, 291, 4414, 6, 7826, 6, 5104, 61, 3, 2, 3, 31635, 11, 169, 13893, 1277, 3, 18, 24083, 8, 13893, 1861, 11, 22545, 45, 8, 5382, 7130, 3595, 11, 3332, 12, 1979, 12, 8, 1218, 5, 1610, 3, 99, 34, 19, 487, 12, 166, 1368, 3, 9, 671, 16, 13893, 5, 37, 29, 161, 30, 8340, 931, 3, 99, 97, 14079, 21, 8, 471, 5, 37, 372, 56, 585, 11, 1708, 8, 433, 21, 671, 5592, 11, 671, 8340, 41, 99, 487, 137, 41, 308, 9, 6961, 6, 66, 372, 61, 2159, 3750, 5, 7171, 460, 2884, 371, 15, 115, 2368, 5545, 23061, 5289, 7, 101, 33, 2718, 13, 175, 982, 10, 3, 2, 101, 341, 103, 59, 1083, 43, 351, 28, 889, 3, 87, 475, 3409, 3, 104, 8718, 13, 8, 13893, 4256, 8340, 313, 19, 8976, 30, 3, 9, 3468, 6093, 28, 514, 475, 592, 5, 71, 418, 13, 69, 1438, 56, 174, 12, 36, 1869, 30, 3, 12663, 12, 482, 46, 12817, 475, 28, 8340, 5644, 5, 611, 6, 62, 33, 230, 479, 44, 1338, 28, 151, 113, 103, 43, 48, 351, 11, 2421, 3, 9, 515, 13, 5660, 5, 3, 2, 13893, 1277, 4432, 2311, 1438, 3, 18, 1875, 8, 372, 47, 1574, 16, 3, 18765, 91, 3, 9, 194, 12, 3170, 13893, 1277, 6, 34, 65, 5723, 24, 16, 455, 12, 1368, 11, 6093, 671, 6, 3, 9, 2021, 11, 2881, 1218, 19, 906, 5, 94, 56, 174, 12, 36, 6262, 78, 24, 34, 54, 36, 2301, 12, 213, 8, 13893, 7724, 33, 1069, 5, 3, 2, 421, 1070, 2231, 87, 7, 17, 2206, 122, 63, 3, 18, 86, 455, 12, 7464, 3, 9, 418, 13, 722, 11, 1759, 1085, 6, 69, 1070, 11, 3662, 56, 174, 12, 36, 24230, 5, 101, 228, 1344, 3, 9, 569, 783, 905, 41, 21246, 5096, 87, 4861, 2567, 61, 11, 87, 127, 726, 21, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [6551, 220, 6357, 26, 884, 10958, 6, 570, 13, 1055, 18599, 1476, 724, 6, 11, 14040, 13893, 1277, 15981, 12, 1115, 8, 8225, 27354, 17, 4313, 336, 471, 5, 377, 10990, 12, 8341, 9216, 79, 857, 79, 43, 6684, 251, 12, 888, 1039, 713, 59, 964, 125, 24, 19, 5, 328, 515, 12, 482, 46, 1046, 3719, 41, 9, 7, 8560, 12, 2772, 53, 38, 7972, 16, 7109, 439, 448, 61, 11, 1368, 3, 9, 13893, 671, 28, 3, 20907, 1277, 5, 328, 2136, 8, 1103, 12, 918, 3, 9, 8340, 313, 1127, 6, 8, 1277, 19, 396, 2881, 5, 328, 916, 12, 43, 8, 337, 982, 68, 278, 22, 17, 43, 1390, 21, 181, 12581, 1222, 5, 2003, 174, 12, 7770, 15, 12, 129, 135, 1149, 1]}], 'validation': [{'input_ids': [21603, 10, 304, 10, 13893, 4256, 2159, 2271, 1029, 10, 1955, 13675, 53, 40, 1647, 19237, 10, 13893, 4256, 2159, 3750, 3, 104, 6551, 7784, 2083, 431, 189, 6, 460, 2884, 638, 8497, 10, 2159, 2271, 6, 17228, 7, 6783, 2197, 101, 33, 464, 30, 175, 12697, 10, 1300, 6514, 1737, 3, 9, 356, 13, 1638, 11, 11361, 372, 724, 5, 1682, 3, 16196, 77, 53, 9216, 12, 8, 13893, 4256, 556, 5, 1877, 3, 23393, 53, 487, 2254, 13, 13893, 8340, 5, 24583, 3, 2092, 48, 41, 7, 14184, 61, 5099, 1059, 6, 62, 10380, 175, 4145, 10, 3, 18, 2588, 305, 9216, 12, 8, 4355, 13893, 4256, 313, 3, 104, 3, 31192, 305, 487, 9216, 41, 17158, 1223, 11, 16335, 61, 12, 13893, 4256, 313, 5, 3, 18, 14170, 21556, 314, 13893, 8340, 3919, 7724, 3, 18, 18961, 314, 1220, 315, 1308, 13, 671, 7724, 41, 21669, 13938, 6, 3, 2, 3229, 16824, 61, 24, 54, 380, 13893, 8340, 5644, 5, 14423, 3, 2092, 8, 416, 471, 6, 62, 515, 12, 1992, 175, 4145, 10, 3, 18, 419, 13536, 69, 2387, 3674, 3, 18, 7772, 81, 6273, 77, 53, 8, 3674, 62, 4802, 336, 471, 5, 14025, 1286, 48, 57, 1577, 220, 13605, 884, 10958, 41, 308, 9, 6961, 6, 5104, 6, 9771, 137, 3, 18, 374, 13536, 69, 18599, 1476, 3, 18, 101, 56, 317, 81, 8, 151, 24, 133, 36, 2690, 21, 178, 12, 1350, 12, 5, 6798, 81, 844, 62, 228, 987, 746, 16, 5, 3, 3626, 175, 4064, 6, 960, 21, 151, 62, 54, 3673, 1350, 12, 11, 2497, 69, 516, 515, 5, 41, 448, 63, 152, 6, 695, 11809, 2271, 137, 3, 18, 2588, 806, 1030, 81, 220, 9216, 3, 18, 1029, 8, 9216, 62, 435, 336, 471, 6, 281, 30, 70, 3395, 11, 2075, 125, 79, 103, 11, 669, 81, 70, 892, 5, 3, 22265, 6, 482, 3, 9, 1708, 57, 2083, 850, 189, 6, 28, 8, 826, 220, 378, 4466, 21, 284, 10, 25758, 87, 26, 99, 11788, 7, 344, 349, 11, 13893, 4256, 6, 8, 1596, 21, 24, 313, 6, 11, 3, 99, 87, 4067, 79, 17123, 5, 41, 439, 291, 4414, 6, 7826, 137, 5289, 7, 101, 33, 2718, 13, 175, 982, 10, 3, 18, 101, 103, 59, 1083, 43, 351, 28, 889, 3, 87, 475, 3409, 3, 104, 8718, 13, 8, 13893, 4256, 8340, 313, 19, 8976, 30, 3, 9, 3468, 6093, 28, 514, 475, 592, 5, 71, 418, 13, 69, 1438, 56, 174, 12, 36, 1869, 30, 3, 12663, 12, 482, 46, 12817, 475, 28, 8340, 5644, 5, 3, 18, 3, 16196, 77, 53, 69, 286, 16, 8, 512, 19, 16114, 3, 18, 37, 372, 3, 28400, 91, 8, 192, 1895, 3212, 3, 18, 352, 12, 2100, 1031, 11, 8340, 135, 30, 1424, 5, 101, 5723, 24, 132, 19, 3, 9, 20374, 628, 16, 8, 512, 21, 13893, 8340, 6, 68, 34, 429, 36, 16114, 12, 15113, 8, 2387, 3674, 12, 6665, 48, 1182, 3, 8911, 8, 717, 5, 3, 18, 14490, 13893, 3919, 1277, 19, 2881, 3, 18, 13893, 19, 341, 3, 9, 4352, 126, 686, 13, 8340, 6, 11, 2459, 8, 512, 21, 48, 748, 19, 341, 4352, 11276, 5, 14490, 1277, 41, 2376, 3, 9, 13893, 3919, 1861, 61, 19, 487, 6, 68, 69, 5971, 13, 3, 2, 3229, 16824, 656, 34, 1256, 5, 3, 18, 421, 1070, 2231, 87, 7, 17, 2206, 122, 63, 19, 3, 9, 750, 682, 62, 43, 5, 86, 455, 12, 7464, 3, 9, 418, 13, 722, 11, 1759, 1085, 6, 69, 1070, 11, 3662, 56, 174, 12, 36, 24230, 5, 101, 228, 1344, 3, 9, 569, 783, 905, 41, 1570, 2427, 5096, 87, 371, 3302, 2567, 61, 11, 87, 127, 726, 21, 3, 9, 1328, 21592, 30, 1424, 5, 2159, 3750, 5, 7171, 460, 2884, 371, 15, 115, 5176, 5545, 209, 16936, 28416, 4128, 3, 18, 14794, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [6551, 204, 18961, 9216, 11, 13893, 8340, 7724, 5, 2926, 30, 3, 19175, 2259, 6, 253, 18599, 1476, 724, 6, 11, 482, 220, 884, 10958, 5, 328, 2136, 8, 1103, 12, 918, 3, 9, 8340, 313, 1127, 6, 8, 1277, 19, 396, 2881, 11, 34, 19, 19363, 113, 70, 722, 33, 24, 133, 726, 21, 34, 5, 3, 21153, 3676, 982, 68, 278, 22, 17, 43, 1390, 21, 181, 12581, 1222, 5, 1]}]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NHpjpVX_VKbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "E40l8l4HTkvC"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "batch_size = 16\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-xsum\",\n",
        "    evaluation_strategy=\"steps\",  # Change this line to use \"steps\" for evaluation strategy\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    push_to_hub=False,\n",
        "    logging_steps=100,\n",
        "    report_to=\"tensorboard\",\n",
        ")"
      ],
      "metadata": {
        "id": "5LK-Vu9tTkxk"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "Z7Lvs5fjrXh7"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "  decoed_labels = [\"\\n\".join(nltk.sent_tokenizer(label.strip())) for label in decoded_labels]\n",
        "\n",
        "  result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "  predictions_lens = [np.count_nonzero(pred != tokenizer.pad_tokenid)for pred in predictions]\n",
        "  result['gen_len'] = np.mean(predictions_lens)\n",
        "\n",
        "  return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "WKZfCh6XrXnL"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "pGE0F6UJrXqR"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "ipz4isbt0YFF",
        "outputId": "00aa3c37-66f7-4260-a9a7-6d9d326fa31a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=5.639340082804362, metrics={'train_runtime': 21.3624, 'train_samples_per_second': 0.421, 'train_steps_per_second': 0.14, 'total_flos': 1601104084992.0, 'train_loss': 5.639340082804362, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "0C1owq_FrXtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6f8aa6-d593-4e0f-9c41-ace905eb8e6b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model and tokenizer\n",
        "model_checkpoint = \"t5-small\"  # Replace with your model checkpoint\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Input text\n",
        "input_text = \"\"\"\n",
        "    To: VRSS Management Team\n",
        "From: David Schlingloff\n",
        "Subject: VRSS Management Report – Week ending March 21st, 2022\n",
        "Copies: Management Team, Advisors\n",
        "Priorities\n",
        "We are working on these priorities:\n",
        "1. Get more people to take our survey and evaluate the results, possibly older folks.\n",
        "2. Plan if we would like to rent out equipment and use it at an intramural game on\n",
        "April 1st.\n",
        "3. Change/ review future plans based on project review feedback.\n",
        "Progress\n",
        "During this reporting period, we accomplished these tasks:\n",
        "● Finalized interest survey results - Synthesized the survey results and added to\n",
        "our report and project review presentation. (Ryan, Matt, Alex)\n",
        "● Obtained and used VR equipment - Obtained the Oculus Quest 2 and\n",
        "connected to YouTube VR.\n",
        "● Performed team project review - Revised presentation template for the class and\n",
        "presented the project review to the team on Wednesday, March 16th. Recorded\n",
        "the comments from the Q&A. (David, all team members)\n",
        "● Created a video for KickStartVT - Recorded and edited a video for KickstartVT\n",
        "and submitted on Sunday, March 20th. (David, all team members)\n",
        "\n",
        "Plans\n",
        "During the next week, we plan to finish these tasks:\n",
        "● Reach more people with survey -Try and reach more people with our survey\n",
        "and try to find a way to reach older folks, possibly interviews. Try to get 3+\n",
        "people for this. (Ryan, Matt, Alex)\n",
        "● Plan to stream a game at an intramural game- We got invited to an intramural\n",
        "game to try and put out equipment and streaming capabilities to the test. So we\n",
        "will plan to see if we can go and rent the equipment and stream. (Kartik, David)\n",
        "● Review project review comments and plan - Review the comments we received\n",
        "from the project review and plan / change future plans by updating the MOKR.\n",
        "(David, all team members)\n",
        "\n",
        "Problems\n",
        "We are aware of these problems:\n",
        "● We can only rent equipment for a limited period - We would like to rent an\n",
        "Oculus Go or an Oculus Quest (since they are stand-alone devices), but they\n",
        "require rentals so timing can get delayed significantly.\n",
        "● We still do not currently have experience with software / website creation –\n",
        "Much of the VRSS streaming service is dependent on a reliable stream with easy\n",
        "website access. A lot of our resources will need to be spent on attempting to\n",
        "create an appealing website with streaming capabilities. However, we are now\n",
        "looking at meeting with people who do have this experience and developing a\n",
        "plan of integration.\n",
        "● Our marketing efforts/strategy - In order to acquire a lot of customers and\n",
        "produce sales, our marketing and advertising will need to be aggressive. We\n",
        "could develop a social media account (instagram/facebook) and/or pay for a\n",
        "commercial advertisement on TV.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize and preprocess the input text\n",
        "inputs = tokenizer(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
        "\n",
        "# Use the model to generate a summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode and print the generated summary\n",
        "generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "id": "Mot7ea1HrXwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9676d5-c7c4-431c-e458-d1afc24c80b4"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            "VRSS Management Report – Week ending March 21st, 2022 Copies: Management Team, Advisors Priorities We are working on these priorities: 1. Get more people to take our survey and evaluate the results, possibly older folks. 3. Plan if we would like to rent out equipment and use it at an intramural game on April 1st.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XoJUzYFE9JMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LD-azS8d9JP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fdlCvin9JTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6xQ-tSE9JXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-QptG-89JbC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}